{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'path_feature' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-6dac04bf44d3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'test_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_quora_features.pkl'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_feature\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0mtest_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'path_feature' is not defined"
     ]
    }
   ],
   "source": [
    "# %load model_weights_cv.py\n",
    "\"\"\"\n",
    "Created on Wed May 17 12:17:12 2017\n",
    "\n",
    "@author: n000153994\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import sparse as ssp\n",
    "\n",
    "def setPath():\n",
    "    if platform.system() == 'Darwin':\n",
    "        path_w2v = '/Volumes/MyPassport/kaggle_quora/w2v_pretrained/'\n",
    "        path_data= '/Volumes/MyPassport/kaggle_quora/data/'\n",
    "        path_feature = '/Volumes/MyPassport/kaggle_quora/features/'\n",
    "        path_unpack = '/Volumes/MyPassport/kaggle_quora/features/un_pack/'\n",
    "        return path_w2v,path_data,path_feature,path_unpack\n",
    "    elif platform.system() == 'Darwin':\n",
    "        path_w2v = 'D:\\\\kaggle_quora\\\\w2v_pretrained\\\\'\n",
    "        path_data= 'D:\\\\kaggle_quora\\\\data\\\\'\n",
    "        path_feature = 'D:\\\\kaggle_quora\\\\features\\\\'\n",
    "        return path_w2v,path_data,path_feature,path_unpack\n",
    "        \n",
    "path_w2v,path_data,path_feature,path_unpack = setPath()\n",
    "\n",
    "\n",
    "# basic features ---- features engineering\n",
    "\n",
    "test_data = pd.DataFrame()\n",
    "\n",
    "for i in range(0,10):\n",
    "    filename = 'test_'+str(i)+'_quora_features.pkl'\n",
    "    data = pd.read_pickle(path_feature+filename)\n",
    "    test_data = test_data.append(data)\n",
    "    \n",
    "    \n",
    "train_data = pd.read_pickle(path_feature + 'train_quora_features.pkl')\n",
    "\n",
    "\n",
    "#train_w2v_q1 = np.load(path_feature+'train_q1_w2v_google.pkl')\n",
    "#train_w2v_q1 = pd.DataFrame(train_w2v_q1,columns=['q1_' + i for i in list(map(str,range(0,train_w2v_q1.shape[1])))])\n",
    "#  \n",
    "#train_w2v_q2 = np.load(path_feature+'train_q2_w2v_google.pkl')\n",
    "#train_w2v_q2 = pd.DataFrame(train_w2v_q2,columns=['q2_' + i for i in list(map(str,range(0,train_w2v_q2.shape[1])))])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# magic features \n",
    "\n",
    "train_comb = pd.read_pickle(path_feature+'magic_feature_train.pkl')\n",
    "test_comb = pd.read_pickle(path_feature+'magic_feature_test.pkl')\n",
    "\n",
    "\n",
    "# features stacking\n",
    " \n",
    "\n",
    "train_data['weights']= [ np.random.uniform(0.51,0.52) if x == 1 else\n",
    "                         np.random.uniform(0.3,0.31) for x in train_data['is_duplicate']]\n",
    "\n",
    "\n",
    "train_features = pd.concat([train_data[train_data.columns.difference(['question1', 'question2'])],\n",
    "                             train_comb[train_comb.columns.difference(['id','is_duplicate'])]], axis=1)\n",
    "    #.tocsr()\n",
    "    \n",
    "\n",
    "test_features = pd.concat([test_data[test_data.columns.difference(['question1', 'question2'])],\n",
    "                            test_comb[test_comb.columns.difference(['id'])]],axis=1)\n",
    "    #.tocsr()\n",
    "    \n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "feature_col = train_features.columns.difference(['id']).values.tolist()\n",
    "\n",
    "pos_train, pos_test = train_test_split(train_features.ix[train_features['is_duplicate']==1,feature_col], test_size = 0.3)\n",
    "neg_train, neg_test = train_test_split(train_features.ix[train_features['is_duplicate']==0,feature_col], test_size = 0.3)\n",
    "\n",
    "train_X = pos_train.append(neg_train)\n",
    "test_X = pos_test.append(neg_test)\n",
    "\n",
    "train_y = train_X.is_duplicate.values\n",
    "weight_X = train_X.weights.values\n",
    "train_X = train_X[train_X.columns.difference(['is_duplicate','weights'])]\n",
    "\n",
    "test_y = test_X.is_duplicate.values\n",
    "weight_x = test_X.weights.values\n",
    "test_X = test_X[test_X.columns.difference(['is_duplicate','weights'])]\n",
    "\n",
    "# Set our parameters for xgboost\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "params = {}\n",
    "params['objective'] = 'binary:logistic'\n",
    "params['eval_metric'] = 'logloss'\n",
    "params['eta'] = 0.02\n",
    "params['max_depth'] = 9\n",
    "\n",
    "d_train = xgb.DMatrix(train_X, label=train_y,weight=weight_X)\n",
    "d_valid = xgb.DMatrix(test_X, label=test_y,weight=weight_x) \n",
    "\n",
    "watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n",
    "\n",
    "\n",
    "bst = xgb.train(params, d_train, 1500, watchlist, early_stopping_rounds=5, verbose_eval=10)\n",
    "               # , feval = kappa)\n",
    "\n",
    "feature_col_test = train_X.columns.values.tolist()\n",
    "\n",
    "d_test = xgb.DMatrix(test_features.ix[:,feature_col_test])\n",
    "p_test = bst.predict(d_test)\n",
    "\n",
    "sub = pd.DataFrame()\n",
    "sub['test_id'] = test_comb['id']\n",
    "sub['is_duplicate'] = p_test\n",
    "\n",
    "sub.to_csv(path_data+'xgb_1805_0.3_0.51_9.csv', index=False)\n",
    "\n",
    "\n",
    "test_comb[test_comb.duplicated(['id'], keep=False)]\n",
    "\n",
    "t = pd.read_csv(path_data+'xgb_1805_0.3_0.51_9.csv')\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
