{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load model_weights_cv.py\n",
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Created on Sun May 14 17:08:27 2017\n",
    "\n",
    "@author: meiyi\n",
    "\"\"\"\n",
    "\n",
    "import platform\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "\n",
    "def setPath():\n",
    "    if platform.system() == 'Darwin':\n",
    "        path_w2v = '/Volumes/MyPassport/kaggle_quora/w2v_pretrained/'\n",
    "        path_data= '/Volumes/MyPassport/kaggle_quora/data/'\n",
    "        path_feature = '/Volumes/MyPassport/kaggle_quora/features/'\n",
    " \n",
    "        return path_w2v,path_data,path_feature \n",
    "    elif platform.system() == 'Windows':\n",
    "        path_w2v = 'D:\\\\kaggle_quora\\\\w2v_pretrained\\\\'\n",
    "        path_data= 'D:\\\\kaggle_quora\\\\data\\\\'\n",
    "        path_feature = 'D:\\\\kaggle_quora\\\\features\\\\'\n",
    "        return path_w2v,path_data,path_feature \n",
    "        \n",
    "path_w2v,path_data,path_feature  = setPath()\n",
    "\n",
    "\n",
    "# basic features ---- features engineering\n",
    "\n",
    "test_data = pd.DataFrame()\n",
    "\n",
    "for i in range(0,10):\n",
    "    filename = 'test_'+str(i)+'_quora_features.pkl'\n",
    "    data = pd.read_pickle(path_feature+filename)\n",
    "    test_data = test_data.append(data)\n",
    "    \n",
    "    \n",
    "train_data = pd.read_pickle(path_feature + 'train_quora_features.pkl')\n",
    "\n",
    "\n",
    "\n",
    "train_porter_intersec = pd.DataFrame(pd.read_pickle(path_feature+'train_porter_interaction.pkl'),\n",
    "                                     columns = ['porter_intersec'])\n",
    "test_porter_intersec = pd.DataFrame(pd.read_pickle(path_feature+'test_porter_interaction.pkl'),\n",
    "                                     columns = ['porter_intersec'])\n",
    "\n",
    "\n",
    "\n",
    "train_intersec = pd.read_pickle(path_feature + 'train_intersect.pkl')\n",
    "test_intersec = pd.read_pickle(path_feature + 'test_intersect.pkl')\n",
    "\n",
    "\n",
    "train_angle = pd.DataFrame(pd.read_pickle(path_feature+'train_q1_q2_angle.pickle'),\n",
    "                                     columns = ['angle'])\n",
    "\n",
    "test_angle = pd.DataFrame(pd.read_pickle(path_feature+'test_q1_q2_angle.pickle'),columns = ['angle'])\n",
    "\n",
    "\n",
    "# magic features \n",
    "\n",
    "train_comb = pd.read_pickle(path_feature+'magic_feature_train.pkl')\n",
    "test_comb = pd.read_pickle(path_feature+'magic_feature_test.pkl')\n",
    "\n",
    "\n",
    "# features stacking\n",
    " \n",
    "\n",
    "train_data['weights']= [ np.random.uniform(0.2,0.21) if x == 1 else\n",
    "                         np.random.uniform(0.8,0.81) for x in train_data['is_duplicate']]\n",
    "\n",
    "\n",
    "train_features = pd.concat([train_data[train_data.columns.difference(['question1', 'question2'])],\n",
    "                                       train_porter_intersec,\n",
    "                                       train_intersec,\n",
    "                                       train_angle,\n",
    "                             train_comb[train_comb.columns.difference(['id','is_duplicate','q1_hash', 'q2_hash'])]], axis=1)\n",
    "  \n",
    "    \n",
    "\n",
    "test_features = pd.concat([test_data[test_data.columns.difference(['question1', 'question2'])],\n",
    "                                     test_porter_intersec,\n",
    "                                     test_intersec,\n",
    "                                     test_angle,\n",
    "                            test_comb[test_comb.columns.difference(['q1_hash', 'q2_hash','id'])]],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['braycurtis_distance', 'canberra_distance', 'cityblock_distance',\n",
      "       'common_words', 'cosine_distance', 'diff_len', 'euclidean_distance',\n",
      "       'fuzz_WRatio', 'fuzz_partial_ratio', 'fuzz_partial_token_set_ratio',\n",
      "       'fuzz_partial_token_sort_ratio', 'fuzz_qratio', 'fuzz_token_set_ratio',\n",
      "       'fuzz_token_sort_ratio', 'jaccard_distance', 'kur_q1vec', 'kur_q2vec',\n",
      "       'len_char_q1', 'len_char_q2', 'len_q1', 'len_q2', 'len_word_q1',\n",
      "       'len_word_q2', 'minkowski_distance', 'norm_wmd', 'skew_q1vec',\n",
      "       'skew_q2vec', 'wmd', 'porter_intersec', 'q1_q2_intersect', 'angle',\n",
      "       'q1_freq', 'q2_freq'],\n",
      "      dtype='object')\n",
      "..1..\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0675efe966b425b959521dd0595061e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..2..\n",
      "\n",
      "..2..\n",
      "\n",
      "..2..\n",
      "\n",
      "..2..\n",
      "\n",
      "..2..\n",
      "\n",
      "\n",
      "..3..\n",
      "\n",
      "33 5 60 18089 26009\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Volumes/MyPassport/kaggle_quora/data/submits/submission_33_cv_5_lr_5_sub_60_18089_26009.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-eeb15e6e13da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     92\u001b[0m }\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m \u001b[0mxgb_models\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-eeb15e6e13da>\u001b[0m in \u001b[0;36mrun_classifier\u001b[0;34m(test_full, train_full, K, run_test_set)\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0mpred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test_id'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_comb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0mpred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'is_duplicate'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_test_pred_fixed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m         \u001b[0mpred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_data\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'submits/submission_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'..4..\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/meiyi/anaconda/lib/python3.5/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mto_csv\u001b[0;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, tupleize_cols, date_format, doublequote, escapechar, decimal)\u001b[0m\n\u001b[1;32m   1381\u001b[0m                                      \u001b[0mdoublequote\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdoublequote\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1382\u001b[0m                                      escapechar=escapechar, decimal=decimal)\n\u001b[0;32m-> 1383\u001b[0;31m         \u001b[0mformatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1385\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpath_or_buf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/meiyi/anaconda/lib/python3.5/site-packages/pandas/formats/format.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1458\u001b[0m             f = _get_handle(self.path_or_buf, self.mode,\n\u001b[1;32m   1459\u001b[0m                             \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1460\u001b[0;31m                             compression=self.compression)\n\u001b[0m\u001b[1;32m   1461\u001b[0m             \u001b[0mclose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1462\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/meiyi/anaconda/lib/python3.5/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36m_get_handle\u001b[0;34m(path, mode, encoding, compression, memory_map)\u001b[0m\n\u001b[1;32m    330\u001b[0m                 \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m                 \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'replace'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Volumes/MyPassport/kaggle_quora/data/submits/submission_33_cv_5_lr_5_sub_60_18089_26009.csv'"
     ]
    }
   ],
   "source": [
    "\n",
    "import xgboost as xgb\n",
    "import pickle\n",
    "from tqdm import tqdm_notebook\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "\n",
    "gamma_0 = 1.30905513329\n",
    "gamma_1 = 0.472008228977\n",
    "\n",
    "def link_function(x):\n",
    "    return gamma_1*x/(gamma_1*x + gamma_0*(1 - x))\n",
    "\n",
    "def link_function_rev(y):\n",
    "    return -((gamma_0 * y) / (gamma_1 * y - gamma_1 - gamma_0 * y))\n",
    "\n",
    "def get_mean_val(models):\n",
    "    return np.mean([model.booster().best_score for model in models])\n",
    "\n",
    "xgb_models = []\n",
    "y_test = []\n",
    "y_train = []\n",
    "\n",
    "def run_classifier(test_full, train_full, K = 5, run_test_set = True):\n",
    "    xgb_models = []\n",
    "    predictors = test_full.columns[:]\n",
    "    print(predictors)\n",
    "    y_test = []\n",
    "    y_train = []\n",
    "    folder = StratifiedKFold(n_splits=K, shuffle=True)\n",
    "    print('..1..\\n')\n",
    "    splits = folder.split(np.zeros(train_full.shape[0]), train_full['is_duplicate'])\n",
    "    for ix_first, ix_second in tqdm_notebook(splits, total=K):\n",
    "        #print (len(ix_first), len(ix_second), len(train_full))\n",
    "        model = xgb.XGBClassifier(silent=True).set_params(**xgb_params)\n",
    "        model = model.fit(train_full.loc[ix_first, predictors], train_full.loc[ix_first, 'is_duplicate'], \n",
    "                              eval_set=[(train_full.loc[ix_second, predictors], train_full.loc[ix_second, 'is_duplicate'])], \n",
    "                              eval_metric='logloss',\n",
    "                              early_stopping_rounds=100, \n",
    "                              verbose=False)\n",
    "        print('..2..\\n')\n",
    "        if run_test_set: \n",
    "            y_test.append(model.predict_proba(test_full[predictors])[:, 1])\n",
    "        y_train.append(model.predict_proba(train_full[predictors])[:, 1])\n",
    "        xgb_models.append(model)\n",
    "\n",
    "    if run_test_set: \n",
    "        y_test_pred = np.array(y_test).T.mean(axis=1)\n",
    "        y_test_pred_fixed = link_function(y_test_pred)\n",
    "    y_train_pred = np.array(y_train).T.mean(axis=1)\n",
    "    train_full['prediction'] = y_train_pred\n",
    "\n",
    "    #make some keys for file save \n",
    "    print('..3..\\n')\n",
    "    lr = str(int(xgb_params['learning_rate'] * 100))\n",
    "    val_score = str(int(get_mean_val(xgb_models) * 100000))\n",
    "    sub_sample = str(int(xgb_params['subsample'] * 100))\n",
    "    predictor_len = str(len(predictors))\n",
    "    train_score = str(int(log_loss(train_full['is_duplicate'].values, train_full['prediction'].values) * 100000))\n",
    "\n",
    "    print(predictor_len, K, sub_sample, train_score, val_score)\n",
    "    key = predictor_len + '_cv_' + str(K) + '_lr_' + lr + '_sub_' + sub_sample + '_' + train_score + '_' + val_score\n",
    "\n",
    "    if run_test_set:\n",
    "        pred = pd.DataFrame()\n",
    "        pred['test_id'] = test_comb['id']\n",
    "        pred['is_duplicate'] = y_test_pred_fixed\n",
    "        pred.to_csv(path_data + 'submits/submission_' + key + '.csv', index=False)\n",
    "    \n",
    "    print('..4..\\n')\n",
    "    train_full[['prediction','is_duplicate']].to_csv(path_data + 'submits/train_prediction_' + key + '.tsv', sep='\\t', index=False)\n",
    "    pickle.dump(xgb_models, open(path_data + 'submits/models_' + key + '.pkl', 'wb'))\n",
    "\n",
    "    return xgb_models\n",
    "\n",
    "\n",
    "\n",
    "xgb_params = {\n",
    "    'max_depth':9, \n",
    "    'learning_rate': 0.05,\n",
    "    'n_estimators': 2500, \n",
    "    'objective': 'binary:logistic',\n",
    "    'nthread': 16, \n",
    "    'gamma': 0, \n",
    "    'subsample': 0.6, \n",
    "    'colsample_bytree': 0.6, \n",
    "    'colsample_bylevel': 1,\n",
    "    'reg_alpha': 0, \n",
    "    'reg_lambda': 1, \n",
    "    'scale_pos_weight': 1\n",
    "}\n",
    "\n",
    "xgb_models = run_classifier(test_features, train_features, 5, True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                0\n",
       "1                1\n",
       "2                2\n",
       "3                3\n",
       "4                4\n",
       "5                5\n",
       "6                6\n",
       "7                7\n",
       "8                8\n",
       "9                9\n",
       "10              10\n",
       "11              11\n",
       "12              12\n",
       "13              13\n",
       "14              14\n",
       "15              15\n",
       "16              16\n",
       "17              17\n",
       "18              18\n",
       "19              19\n",
       "20              20\n",
       "21              21\n",
       "22              22\n",
       "23              23\n",
       "24              24\n",
       "25              25\n",
       "26              26\n",
       "27              27\n",
       "28              28\n",
       "29              29\n",
       "            ...   \n",
       "2345766    2345766\n",
       "2345767    2345767\n",
       "2345768    2345768\n",
       "2345769    2345769\n",
       "2345770    2345770\n",
       "2345771    2345771\n",
       "2345772    2345772\n",
       "2345773    2345773\n",
       "2345774    2345774\n",
       "2345775    2345775\n",
       "2345776    2345776\n",
       "2345777    2345777\n",
       "2345778    2345778\n",
       "2345779    2345779\n",
       "2345780    2345780\n",
       "2345781    2345781\n",
       "2345782    2345782\n",
       "2345783    2345783\n",
       "2345784    2345784\n",
       "2345785    2345785\n",
       "2345786    2345786\n",
       "2345787    2345787\n",
       "2345788    2345788\n",
       "2345789    2345789\n",
       "2345790    2345790\n",
       "2345791    2345791\n",
       "2345792    2345792\n",
       "2345793    2345793\n",
       "2345794    2345794\n",
       "2345795    2345795\n",
       "Name: id, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_comb['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
